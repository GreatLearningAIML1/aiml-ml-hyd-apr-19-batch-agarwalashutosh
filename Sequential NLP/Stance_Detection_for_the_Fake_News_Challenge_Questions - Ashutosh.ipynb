{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stance_Detection_for_the_Fake_News_Challenge_Questions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbzqLDNeqGXs",
        "colab_type": "text"
      },
      "source": [
        "# **The Real Problem**\n",
        "\n",
        "The goal of the Fake News Challenge is to explore how artificial intelligence technologies, particularly machine learning and natural language processing, might be leveraged to combat the fake news problem. We believe that these AI technologies hold promise for significantly automating parts of the procedure human fact checkers use today to determine if a story is real or a hoax. h\n",
        " \n",
        "Assessing the veracity of a news story is a complex and cumbersome task, even for trained experts. Fortunately, the process can be broken down into steps or stages. A helpful first step towards identifying fake news is to understand what other news organizations are saying about the topic. We believe automating this process, called Stance Detection, could serve as a useful building block in an AI-assisted fact-checking pipeline. \n",
        " \n",
        "Stance Detection involves estimating the relative perspective (or stance) of two pieces of text relative to a topic, claim or issue. This extends the work of Ferreira & Vlachos. The task is to estimate the stance of a body text from a news article relative to a headline. Specifically, the body text may agree, disagree, discuss or be unrelated to the headline. \n",
        " \n",
        "# **Project Description **\n",
        "\n",
        "Given the huge impact of social networks, online content plays an important role in forming or changing the opinions of people. Unlike traditional journalism where only certain news organizations can publish content, online journalism has given chance even for individuals to publish. This has its own advantages like individual empowerment but has given a chance to a lot of malicious entities to spread misinformation for their own benefit. As reported by many organizations in recent history, this even has an influence on major events like the outcome of elections. Therefore, it is of great importance now, to have some sort of automated classification of news stories. \t \t \t \t \n",
        " \n",
        "In this work, we will implement different deep neural architectures using word embeddings for identifying the stance detection of different news stories. \n",
        "\n",
        "# **Dataset**\n",
        "\n",
        "The dataset is taken from Fake News Challenge, where the goal of the challenge is not to directly identify whether a headline or article is “fake” or not, which is arguably a highly subjective question, and one that even skilled humans may have difficulty answering. \n",
        " \n",
        "The challenge is organized around the more well-defined problem of “stance detection,” which involves comparing a headline with a body of text from a news article to determine what relationship (if any) exists between the two.  \n",
        " \n",
        "There are 4 possible classifications:  \n",
        " \n",
        "1.\tThe article text agrees with the headline.  \n",
        "2.\tThe article text disagrees with the headline.  \n",
        "3.\tThe article text is a discussion of the headline, without taking a position on it.\t \n",
        "4.\tThe article text is unrelated to the headline (i.e. it doesn’t address the same topic).  \n",
        " \n",
        " \n",
        "Presumably, a classifier that can solve the stance detection problem with high accuracy might effectively be used either as a tool for humans working to identify fake news (e.g., retrieving articles that agree, disagree and discuss a headline), or as a building block for a more elaborate future AI system that would try to identify the actual truthfulness of news stories (e.g., using credible sources to classify).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YadB2bsFqjx8",
        "colab_type": "text"
      },
      "source": [
        "# **I will follow these steps: **\n",
        "\n",
        "Step 1: Understanding how to convert words into vectors and make them ready for the model. \n",
        "This is an important step and will be the same for almost any problem in NLP when dealing with words. \n",
        " \n",
        "Step 2: Understanding the working of Sequential Models like RNN, LSTM.. and practically building them in Keras and build the model ready. \n",
        " \n",
        "Step 3: Train the model and report the accuracy score. \n",
        " \n",
        "Step 4: Add checkpoints as callbacks to the model and save the model for each epoch. This callback saves the model after each epoch, which can be handy for when you are running long-term training. \n",
        "By loading the saved weight files we can start training the model which starts the learning from there. \n",
        "\n",
        "Step 5 (will try if time permits): Add attention to the above model and report the change in accuracy. And explain how and why attention has improved (or disturbed) the model. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI9jhXKPCFcJ",
        "colab_type": "text"
      },
      "source": [
        "# Stance Detection for the Fake News Challenge\n",
        "\n",
        "## Identifying Textual Relationships with Deep Neural Nets\n",
        "\n",
        "### Check the problem context [here](https://drive.google.com/open?id=1KfWaZyQdGBw8AUTacJ2yY86Yxgw2Xwq0).\n",
        "\n",
        "### Download files required for the project from [here](https://drive.google.com/open?id=10yf39ifEwVihw4xeJJR60oeFBY30Y5J8)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSNgdEMpenpE",
        "colab_type": "text"
      },
      "source": [
        "## Step1: Load the given dataset  \n",
        "\n",
        "1. Mount the google drive\n",
        "\n",
        "2. Import Glove embeddings\n",
        "\n",
        "3. Import the test and train datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjUOqplktx6s",
        "colab_type": "text"
      },
      "source": [
        "**Mount the google drive to access required project files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1TmwYOxrg-3",
        "colab_type": "code",
        "outputId": "1aa1ec35-019c-4047-ce8f-3a35fece3a25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhZdJ4zpwWzN",
        "colab_type": "text"
      },
      "source": [
        "#### Path for Project files on google drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aol97RUogFuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "project_path = \"/content/drive/My Drive/GreatLakes/Artificial Intelligence/Sequence Models in NLP/Project2-FakeNewsDetection/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ly0VxAnwJ2f",
        "colab_type": "text"
      },
      "source": [
        "### Loading the Glove Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3ZXifofoNz_",
        "colab_type": "text"
      },
      "source": [
        "I have commented this code as the files have been unzipped once and not required with every code run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmsPn6PF-cgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#from zipfile import ZipFile\n",
        "#with ZipFile(project_path+'glove.6B.zip', 'r') as z:\n",
        " # z.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjLJEQ_PwcGi",
        "colab_type": "text"
      },
      "source": [
        "# Load the dataset \n",
        "\n",
        "1. Using [read_csv()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) in pandas load the given train datasets files **`train_bodies.csv`** and **`train_stances.csv`**\n",
        "\n",
        "2. Using [merge](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) command in pandas merge the two datasets based on the Body ID. \n",
        "\n",
        "Note: Save the final merged dataset in a dataframe with name **`dataset`**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gXO1WZ-gFwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kosAWskdOOT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_bodies = pd.read_csv(project_path+'train_bodies.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siXsJbIbyTrm",
        "colab_type": "code",
        "outputId": "d8ac0ae0-54ea-4125-e18e-cfd3a5d74805",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df_bodies.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>articleBody</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>Last week we hinted at what was to come as Ebo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>(NEWSER) – Wonder how long a Quarter Pounder w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>Posting photos of a gun-toting child online, I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>At least 25 suspected Boko Haram insurgents we...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Body ID                                        articleBody\n",
              "0        0  A small meteorite crashed into a wooded area i...\n",
              "1        4  Last week we hinted at what was to come as Ebo...\n",
              "2        5  (NEWSER) – Wonder how long a Quarter Pounder w...\n",
              "3        6  Posting photos of a gun-toting child online, I...\n",
              "4        7  At least 25 suspected Boko Haram insurgents we..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMPQX7dKyJUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_stances = pd.read_csv(project_path+'train_stances.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReldYOUMyemt",
        "colab_type": "code",
        "outputId": "35147107-850e-42e4-f661-8c45d855a082",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df_stances.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Body ID</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Police find mass graves with at least '15 bodi...</td>\n",
              "      <td>712</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hundreds of Palestinians flee floods in Gaza a...</td>\n",
              "      <td>158</td>\n",
              "      <td>agree</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Christian Bale passes on role of Steve Jobs, a...</td>\n",
              "      <td>137</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HBO and Apple in Talks for $15/Month Apple TV ...</td>\n",
              "      <td>1034</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Spider burrowed through tourist's stomach and ...</td>\n",
              "      <td>1923</td>\n",
              "      <td>disagree</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Headline  Body ID     Stance\n",
              "0  Police find mass graves with at least '15 bodi...      712  unrelated\n",
              "1  Hundreds of Palestinians flee floods in Gaza a...      158      agree\n",
              "2  Christian Bale passes on role of Steve Jobs, a...      137  unrelated\n",
              "3  HBO and Apple in Talks for $15/Month Apple TV ...     1034  unrelated\n",
              "4  Spider burrowed through tourist's stomach and ...     1923   disagree"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlCRaBBFyik4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df_bodies.merge(df_stances, left_on='Body ID', right_on='Body ID')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4ycQbBCg20S",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<h2> Run `dataset.head() and Check the output </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUtF7iOmj11k",
        "colab_type": "code",
        "outputId": "08a71322-06eb-4655-f38b-005d40cd8d0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>articleBody</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Soldier shot, Parliament locked down after gun...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Tourist dubbed ‘Spider Man’ after spider burro...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Luke Somers 'killed in failed rescue attempt i...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>BREAKING: Soldier shot at War Memorial in Ottawa</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Giant 8ft 9in catfish weighing 19 stone caught...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Body ID  ...     Stance\n",
              "0        0  ...  unrelated\n",
              "1        0  ...  unrelated\n",
              "2        0  ...  unrelated\n",
              "3        0  ...  unrelated\n",
              "4        0  ...  unrelated\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjzVz2ifijmj",
        "colab_type": "text"
      },
      "source": [
        "## Step2: Data Pre-processing and setting some hyper parameters needed for model\n",
        "\n",
        "\n",
        "#### lets set the following required parameters.\n",
        "\n",
        "1. `MAX_SENTS` = Maximum no.of sentences to consider in an article.\n",
        "\n",
        "2. `MAX_SENT_LENGTH` = Maximum no.of words to consider in a sentence.\n",
        "\n",
        "3. `MAX_NB_WORDS` = Maximum no.of words in the total vocabualry.\n",
        "\n",
        "4. `MAX_SENTS_HEADING` = Maximum no.of sentences to consider in a heading of an article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDXSdpvqjuqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_NB_WORDS = 20000\n",
        "MAX_SENTS = 20\n",
        "MAX_SENTS_HEADING = 1\n",
        "MAX_SENT_LENGTH = 20\n",
        "VALIDATION_SPLIT = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwE7CPHdiDT-",
        "colab_type": "text"
      },
      "source": [
        "### Download the `Punkt` from nltk using the commands given below. This is for sentence tokenization.\n",
        "\n",
        "reference: read  [this](https://stackoverflow.com/questions/35275001/use-of-punktsentencetokenizer-in-nltk).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsiKmyJUZ-hU",
        "colab_type": "code",
        "outputId": "acaf99ad-c89a-4129-e99a-ddfb84396ff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gqwm_GbwwnhX",
        "colab_type": "text"
      },
      "source": [
        "# Tokenizing the text and loading the pre-trained Glove word embeddings for each token "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfZLR24mm32k",
        "colab_type": "text"
      },
      "source": [
        "Keras provides [Tokenizer API](https://keras.io/preprocessing/text/) for preparing text. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLSn9S-5oG4Z",
        "colab_type": "text"
      },
      "source": [
        "#### Import the Tokenizer from keras preprocessing text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-VUgh2yoMlR",
        "colab_type": "code",
        "outputId": "e53258b7-d001-4e4f-f045-5a0850d5fd2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eml0Lge4oOuh",
        "colab_type": "text"
      },
      "source": [
        "#### Initialize the Tokenizer class with maximum vocabulary count as `MAX_NB_WORDS` initialized at the start of step2. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm85qirPofc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_NB_WORDS) # num_words -> Vocablury size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBe1KuXDosJ7",
        "colab_type": "text"
      },
      "source": [
        "#### Now, using fit_on_texts() from Tokenizer class, lets encode the data \n",
        "\n",
        "Note: We need to fit articleBody and Headline also to cover all the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5rk-UyBlmyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t.fit_on_texts(df['articleBody']) \n",
        "t.fit_on_texts(df['Headline'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omptHX-JpBsN",
        "colab_type": "text"
      },
      "source": [
        "#### fit_on_texts() gives the following attributes in the output as given [here](https://faroit.github.io/keras-docs/1.2.2/preprocessing/text/).\n",
        "\n",
        "* **word_counts:** dictionary mapping words (str) to the number of times they appeared on during fit. Only set after fit_on_texts was called.\n",
        "\n",
        "* **word_docs:** dictionary mapping words (str) to the number of documents/texts they appeared on during fit. Only set after fit_on_texts was called.\n",
        "\n",
        "* **word_index:** dictionary mapping words (str) to their rank/index (int). Only set after fit_on_texts was called.\n",
        "\n",
        "* **document_count:** int. Number of documents (texts/sequences) the tokenizer was trained on. Only set after fit_on_texts or fit_on_sequences was called.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jxaxmkz-MbKO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4dd2e4a5-573e-4c25-8133-8e31bca1b4a4"
      },
      "source": [
        "\n",
        "len(t.word_counts)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27873"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqXcfLPZ-PBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHnsT2sTtFAA",
        "colab_type": "text"
      },
      "source": [
        "### Now, tokenize the sentences using nltk sent_tokenize() and encode the senteces with the ids we got form the above `t.word_index`\n",
        "\n",
        "Initialise 2 lists with names `texts` and `articles`.\n",
        "\n",
        "```\n",
        "texts = [] to store text of article as it is.\n",
        "\n",
        "articles = [] split the above text into a list of sentences.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctEu-d4c4EZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = [] \n",
        "articles = [] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq6da_8vWkYO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = df['articleBody']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIqWzjwh6i2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for text in texts:\n",
        "    articles.append(sent_tokenize(text))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koTVJjoO6P78",
        "colab_type": "text"
      },
      "source": [
        "## Check 2:\n",
        "\n",
        "first element of texts and articles should be as given below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mWBW99p5UW9",
        "colab_type": "code",
        "outputId": "e099ee77-2c5c-4d06-9ba7-1acb466be2c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "texts[0]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A small meteorite crashed into a wooded area in Nicaragua\\'s capital of Managua overnight, the government said Sunday. Residents reported hearing a mysterious boom that left a 16-foot deep crater near the city\\'s airport, the Associated Press reports. \\n\\nGovernment spokeswoman Rosario Murillo said a committee formed by the government to study the event determined it was a \"relatively small\" meteorite that \"appears to have come off an asteroid that was passing close to Earth.\" House-sized asteroid 2014 RC, which measured 60 feet in diameter, skimmed the Earth this weekend, ABC News reports. \\nMurillo said Nicaragua will ask international experts to help local scientists in understanding what happened.\\n\\nThe crater left by the meteorite had a radius of 39 feet and a depth of 16 feet,  said Humberto Saballos, a volcanologist with the Nicaraguan Institute of Territorial Studies who was on the committee. He said it is still not clear if the meteorite disintegrated or was buried.\\n\\nHumberto Garcia, of the Astronomy Center at the National Autonomous University of Nicaragua, said the meteorite could be related to an asteroid that was forecast to pass by the planet Saturday night.\\n\\n\"We have to study it more because it could be ice or rock,\" he said.\\n\\nWilfried Strauch, an adviser to the Institute of Territorial Studies, said it was \"very strange that no one reported a streak of light. We have to ask if anyone has a photo or something.\"\\n\\nLocal residents reported hearing a loud boom Saturday night, but said they didn\\'t see anything strange in the sky.\\n\\n\"I was sitting on my porch and I saw nothing, then all of a sudden I heard a large blast. We thought it was a bomb because we felt an expansive wave,\" Jorge Santamaria told The Associated Press.\\n\\nThe site of the crater is near Managua\\'s international airport and an air force base. Only journalists from state media were allowed to visit it.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlOA1FZ4OZUR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "7bfe0911-5e82-4919-ff8b-77dd869e952d"
      },
      "source": [
        "articles[0]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"A small meteorite crashed into a wooded area in Nicaragua's capital of Managua overnight, the government said Sunday.\",\n",
              " \"Residents reported hearing a mysterious boom that left a 16-foot deep crater near the city's airport, the Associated Press reports.\",\n",
              " 'Government spokeswoman Rosario Murillo said a committee formed by the government to study the event determined it was a \"relatively small\" meteorite that \"appears to have come off an asteroid that was passing close to Earth.\"',\n",
              " 'House-sized asteroid 2014 RC, which measured 60 feet in diameter, skimmed the Earth this weekend, ABC News reports.',\n",
              " 'Murillo said Nicaragua will ask international experts to help local scientists in understanding what happened.',\n",
              " 'The crater left by the meteorite had a radius of 39 feet and a depth of 16 feet,  said Humberto Saballos, a volcanologist with the Nicaraguan Institute of Territorial Studies who was on the committee.',\n",
              " 'He said it is still not clear if the meteorite disintegrated or was buried.',\n",
              " 'Humberto Garcia, of the Astronomy Center at the National Autonomous University of Nicaragua, said the meteorite could be related to an asteroid that was forecast to pass by the planet Saturday night.',\n",
              " '\"We have to study it more because it could be ice or rock,\" he said.',\n",
              " 'Wilfried Strauch, an adviser to the Institute of Territorial Studies, said it was \"very strange that no one reported a streak of light.',\n",
              " 'We have to ask if anyone has a photo or something.\"',\n",
              " \"Local residents reported hearing a loud boom Saturday night, but said they didn't see anything strange in the sky.\",\n",
              " '\"I was sitting on my porch and I saw nothing, then all of a sudden I heard a large blast.',\n",
              " 'We thought it was a bomb because we felt an expansive wave,\" Jorge Santamaria told The Associated Press.',\n",
              " \"The site of the crater is near Managua's international airport and an air force base.\",\n",
              " 'Only journalists from state media were allowed to visit it.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpuRIA7cCfcY",
        "colab_type": "text"
      },
      "source": [
        "# Now iterate through each article and each sentence to encode the words into ids using t.word_index  [5 marks] \n",
        "\n",
        "Here, to get words from sentence you can use `text_to_word_sequence` from keras preprocessing text.\n",
        "\n",
        "1. Import text_to_word_sequence\n",
        "\n",
        "2. Initialize a variable of shape (no.of articles, MAX_SENTS, MAX_SENT_LENGTH) with name `data` with zeros first (you can use numpy [np.zeros](https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html) to initialize with all zeros)and then update it while iterating through the words and sentences in each article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVyClBULCqWj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5bc4d20a-9361-4623-aa82-068f698e4fea"
      },
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va7rJvEToMuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_NB_WORDS = 20000\n",
        "MAX_SENTS = 20\n",
        "MAX_SENTS_HEADING = 1\n",
        "MAX_SENT_LENGTH = 20\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9oKA6ukpFd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data =  np.zeros((len(articles), MAX_SENTS, MAX_SENT_LENGTH), dtype = int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61gUsUgMyZUr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d2d5affc-0d0b-4a86-bf96-3a178a47235e"
      },
      "source": [
        "print (len(articles))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5Wu2tf7pVhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenizing the text \n",
        "\n",
        "for index_one, article in enumerate(articles):\n",
        "  article = article[0:MAX_SENTS]\n",
        "  for index_two, sentence in enumerate(article):\n",
        "    list_of_words = text_to_word_sequence(sentence)[0:MAX_SENT_LENGTH]\n",
        "    encodings = []\n",
        "    \n",
        "    for word in list_of_words:\n",
        "      encoding = t.word_index.get(word)\n",
        "      encodings.append(encoding)\n",
        "    if(len(encodings) < MAX_SENT_LENGTH):\n",
        "      encodings.extend([0] * (MAX_SENT_LENGTH - len(encodings)))\n",
        "    data[index_one][index_two] = encodings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFdmiDYcE144",
        "colab_type": "text"
      },
      "source": [
        "### Check 3:\n",
        "\n",
        "Accessing first element in data should give something like given below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsFWW5C2Djog",
        "colab_type": "code",
        "outputId": "df844403-c44f-447e-8522-c4ea0aa94700",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "data[0, :, :]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    3,   481,   427,  7211,    81,     3,  3733,   331,     5,\n",
              "         3891,   350,     4,  1431,  2958,     1,    89,    12,   464,\n",
              "            0,     0],\n",
              "       [  758,    95,  1047,     3,  2679,  1752,     7,   189,     3,\n",
              "         1217,  1075,  2030,   700,   159,     1,  3032,   448,     1,\n",
              "          555,   235],\n",
              "       [   89,  1067,  4115,  2349,    12,     3,  1092,  3306,    19,\n",
              "            1,    89,     2,  1793,     1,   521,  2009,    15,     9,\n",
              "            3,  3111],\n",
              "       [  181,  3640,   972,   200,  2556,    44,  6775,  1722,  1252,\n",
              "            5, 13317, 17936,     1,   778,    31,   740,  3990,    67,\n",
              "           85,     0],\n",
              "       [ 2349,    12,  1557,    38,  1094,   351,   775,     2,   367,\n",
              "          260,  1770,     5,  4450,    70,   494,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [    1,   700,   189,    19,     1,   427,    32,     3,  7417,\n",
              "            4,  2159,  1252,     6,     3,  5270,     4,  1217,  1252,\n",
              "           12,  3363],\n",
              "       [   13,    12,    15,     8,   149,    25,   543,    64,     1,\n",
              "          427,  3727,    41,     9,  1850,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [ 3363,  5733,     4,     1,  5875,   614,    21,     1,   311,\n",
              "         3438,   794,     4,  1557,    12,     1,   427,    69,    23,\n",
              "          787,     2],\n",
              "       [   37,    17,     2,  1793,    15,    52,   120,    15,    69,\n",
              "           23,  4921,    41,  1963,    13,    12,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [ 4736,  3338,    24,  3969,     2,     1,  1316,     4,  3072,\n",
              "         1653,    12,    15,     9,   195,  1420,     7,    58,    40,\n",
              "           95,     3],\n",
              "       [   37,    17,     2,  1094,    64,   510,    20,     3,   250,\n",
              "           41,   264,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [  260,   758,    95,  1047,     3,  1806,  1752,   531,   276,\n",
              "           29,    12,    33,   703,   163,   892,  1420,     5,     1,\n",
              "         2081,     0],\n",
              "       [   35,     9,  2057,    10,   116,  5825,     6,    35,   576,\n",
              "          656,   104,    59,     4,     3,  2410,    35,   241,     3,\n",
              "          512,  1911],\n",
              "       [   37,   341,    15,     9,     3,  2082,   120,    37,   881,\n",
              "           24,  4451,  2584,  4315,  4922,    55,     1,   555,   235,\n",
              "            0,     0],\n",
              "       [    1,   255,     4,     1,   700,     8,   159,  3961,   351,\n",
              "          448,     6,    24,   155,   465,  1929,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [  126,   921,    22,    47,   100,    36,  1833,     2,  1212,\n",
              "           15,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTG6JySHehkT",
        "colab_type": "text"
      },
      "source": [
        "## Repeat the same process for the `Headings` as well. Use variables with names `texts_heading` and `articles_heading` accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CliiIhLemJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts_heading = df.Headline\n",
        "articles_heading = []\n",
        "for text_heading in texts_heading:\n",
        "  article_heading = nltk.sent_tokenize(text_heading)\n",
        "  articles_heading.append(article_heading)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ4gFNA5A6bx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed17d611-e636-4743-d405-95d85b2b154f"
      },
      "source": [
        "texts_heading[0]\n",
        "articles_heading[0]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Soldier shot, Parliament locked down after gunfire erupts at war memorial']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNqqHSwzBAo1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca479cc6-5e68-450d-e161-e11965124a98"
      },
      "source": [
        "print(len(articles_heading))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yztwfmmzr0SA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_heading =  np.zeros((len(articles_heading), MAX_SENTS_HEADING, MAX_SENT_LENGTH), dtype = int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yaMnJADr1nI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for index_one, heading in enumerate(articles_heading):\n",
        "  heading = heading[0:MAX_SENTS_HEADING]\n",
        "  for index_two, head in enumerate(heading):\n",
        "    list_of_words = text_to_word_sequence(head)[0:MAX_SENT_LENGTH]\n",
        "    encodings = []\n",
        "    for word in list_of_words:\n",
        "      encoding = t.word_index.get(word)\n",
        "      encodings.append(encoding)\n",
        "    if(len(encodings) < MAX_SENT_LENGTH):\n",
        "      encodings.extend([0] * (MAX_SENT_LENGTH - len(encodings)))\n",
        "    data_heading[index_one][index_two] = encodings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOVwoKDPBFkj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0fc66e72-27d4-4200-c2da-e12b3ad9f54f"
      },
      "source": [
        "data_heading[49971, :, :]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6105, 6363, 2165, 2157, 1856, 1998,   14,  284,  145, 2379,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaH0Ey1qe_Co",
        "colab_type": "text"
      },
      "source": [
        "### Now the features are ready, lets make the labels ready for the model to process.\n",
        "\n",
        "### Convert labels into one-hot vectors\n",
        "\n",
        "You can use [get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) in pandas to create one-hot vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq-VcgM8fat1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = pd.get_dummies(df_stances.Stance)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ffP5HdCBmeM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6b5c4206-3795-4d59-c9d7-fa049da1a57e"
      },
      "source": [
        "labels.head()\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>agree</th>\n",
              "      <th>disagree</th>\n",
              "      <th>discuss</th>\n",
              "      <th>unrelated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   agree  disagree  discuss  unrelated\n",
              "0      0         0        0          1\n",
              "1      1         0        0          0\n",
              "2      0         0        0          1\n",
              "3      0         0        0          1\n",
              "4      0         1        0          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40mA8FI2fcxZ",
        "colab_type": "text"
      },
      "source": [
        "### Check 4:\n",
        "\n",
        "The shape of data and labels shoould match the given below numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpEWEnjFfnFR",
        "colab_type": "code",
        "outputId": "356c3d21-78bd-48c9-d8f2-513f82d0c378",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (49972, 20, 20)\n",
            "Shape of label tensor: (49972, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDOxHdR3frDu",
        "colab_type": "text"
      },
      "source": [
        "### Shuffle the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ra-yYTvfzRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## get numbers upto no.of articles\n",
        "indices = np.arange(data.shape[0])\n",
        "## shuffle the numbers\n",
        "np.random.shuffle(indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKnSqwIFf3Iy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## shuffle the data\n",
        "data = data[indices]\n",
        "data_heading = data_heading[indices]\n",
        "## shuffle the labels according to data\n",
        "#labels = labels[indices]\n",
        "labels = labels.iloc[indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcOFVfPBf9kA",
        "colab_type": "text"
      },
      "source": [
        "### Split into train and validation sets. Split the train set 80:20 ratio to get the train and validation sets.\n",
        "\n",
        "\n",
        "Use the variable names as given below:\n",
        "\n",
        "x_train, x_val - for body of articles.\n",
        "\n",
        "x-heading_train, x_heading_val - for heading of articles.\n",
        "\n",
        "y_train - for training labels.\n",
        "\n",
        "y_val - for validation labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2neh9Wcof8iR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pCVc2SrzuQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5u3PTz3gEV-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f19ca89d-b972-4d2b-e28e-ac3f43c15024"
      },
      "source": [
        "int(len(data) * 0.8)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39977"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV0Pc70eCF0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = data[0 : int(len(data) * 0.8) + 1, :, :]\n",
        "x_val = data[int(len(data) * 0.8) + 1 : len(data), :, :]\n",
        "\n",
        "x_heading_train = data_heading[0 : int(len(data_heading) * 0.8) + 1, :, :]\n",
        "x_heading_val = data_heading[int(len(data_heading) * 0.8) + 1 : len(data_heading), :, :]\n",
        "\n",
        "y_train = labels.iloc[0:int(len(data) * 0.8) + 1, :]\n",
        "y_val = labels.iloc[int(len(data) * 0.8) + 1 : len(data), :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zny7WYo3COED",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "121b3150-563b-490b-fada-2ee0d6614708"
      },
      "source": [
        "print(x_val.shape)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9994, 20, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN5vVtzoCQwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTyvoHrsgMDw",
        "colab_type": "text"
      },
      "source": [
        "### Check 5:\n",
        "\n",
        "The shape of x_train, x_val, y_train and y_val should match the below numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLEbiw2Yghe2",
        "colab_type": "code",
        "outputId": "dddc3162-4cb6-43ab-ac93-c89feeb385d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(39978, 20, 20)\n",
            "(39978, 4)\n",
            "(9994, 20, 20)\n",
            "(9994, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baXUlnNE0BSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.reshape(x_train, (39977, 400))\n",
        "x_val = np.reshape(x_val, (9995, 400))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmHw6ZO20EYJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "14bb5f8b-225d-47b7-ecb7-9148eb5b7652"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(39977, 400)\n",
            "(39977, 4)\n",
            "(9995, 400)\n",
            "(9995, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNnoBtArhJ1E",
        "colab_type": "text"
      },
      "source": [
        "### Create embedding matrix with the glove embeddings\n",
        "\n",
        "\n",
        "create embedding_matrix which has all the words and their glove embedding if present in glove word list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKqn2IL2ZF8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # load the whole embedding into memory\n",
        "# embeddings_index = dict()\n",
        "# f = open('./glove.6B.100d.txt')\n",
        "# for line in f:\n",
        "# \tvalues = line.split()\n",
        "# \tword = values[0]\n",
        "# \tcoefs = np.asarray(values[1:], dtype='float32')\n",
        "# \tembeddings_index[word] = coefs\n",
        "# f.close()\n",
        "# print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# # create a weight matrix for words in training docs\n",
        "# embedding_matrix = np.zeros((vocab_size, 100))\n",
        "\n",
        "\n",
        "# for word, i in t.word_index.items():\n",
        "# \tembedding_vector = embeddings_index.get(word)\n",
        "# \tif embedding_vector is not None:\n",
        "# \t\tembedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCTTGovJCZ2f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1aa6fc42-73f4-4ac5-950a-ab72382b0618"
      },
      "source": [
        "\t\t# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "f = open( project_path +  'glove6B/glove.6B.100d.txt')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((MAX_NB_WORDS, 100))\n",
        "\n",
        "\n",
        "for word, i in t.word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "      if(i >= MAX_NB_WORDS):\n",
        "        continue\n",
        "      else:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRi4o3ZspDFU",
        "colab_type": "text"
      },
      "source": [
        "# Try the sequential model approach and report the accuracy score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSZDnPWkw2ZZ",
        "colab_type": "text"
      },
      "source": [
        "### Import layers from Keras to build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AgwQsfMrzAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional, GlobalAveragePooling1D\n",
        "\n",
        "tf.set_random_seed(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpkVhIbx3gr1",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_8QXh-rmPFq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "7c095a3d-62d1-477d-e3f6-751e6e37c137"
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(len(t.word_index) + 1, 100))\n",
        "model.add(keras.layers.GlobalAveragePooling1D())\n",
        "model.add(keras.layers.Dense(64, activation='relu'))\n",
        "\n",
        "# Dropout for regularization\n",
        "model.add(keras.layers.Dropout(0.5))\n",
        "model.add(keras.layers.Dense(32, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(16, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(4, activation=\"sigmoid\"))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfGeMHUxwMBh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "d26139a3-a17d-4666-f2dc-10ff2b588ac5"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 100)         2787400   \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                6464      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 4)                 68        \n",
            "=================================================================\n",
            "Total params: 2,796,540\n",
            "Trainable params: 2,796,540\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnF3leTEDdWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"init_model.h5\", monitor='loss', verbose=1, save_best_only=True, mode='auto')\n",
        "early = EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=1, mode='auto')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5Xrd-JQ3id7",
        "colab_type": "text"
      },
      "source": [
        "### Compile and fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlduHU2CovxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM3yCmjQoCM3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "6b2821fd-3e0f-4da6-ecde-437684d79778"
      },
      "source": [
        "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs = 10, batch_size = 50, verbose = 1)## Train the model"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 39977 samples, validate on 9995 samples\n",
            "Epoch 1/10\n",
            "39977/39977 [==============================] - 7s 164us/sample - loss: 0.8346 - acc: 0.7294 - val_loss: 0.7841 - val_acc: 0.7367\n",
            "Epoch 2/10\n",
            "39977/39977 [==============================] - 5s 118us/sample - loss: 0.8052 - acc: 0.7300 - val_loss: 0.7857 - val_acc: 0.7367\n",
            "Epoch 3/10\n",
            "39977/39977 [==============================] - 5s 126us/sample - loss: 0.8015 - acc: 0.7300 - val_loss: 0.7870 - val_acc: 0.7367\n",
            "Epoch 4/10\n",
            "39977/39977 [==============================] - 5s 119us/sample - loss: 0.7987 - acc: 0.7300 - val_loss: 0.7867 - val_acc: 0.7367\n",
            "Epoch 5/10\n",
            "39977/39977 [==============================] - 5s 122us/sample - loss: 0.7971 - acc: 0.7300 - val_loss: 0.7917 - val_acc: 0.7367\n",
            "Epoch 6/10\n",
            "39977/39977 [==============================] - 5s 115us/sample - loss: 0.7950 - acc: 0.7300 - val_loss: 0.7899 - val_acc: 0.7367\n",
            "Epoch 7/10\n",
            "39977/39977 [==============================] - 5s 116us/sample - loss: 0.7940 - acc: 0.7300 - val_loss: 0.7901 - val_acc: 0.7367\n",
            "Epoch 8/10\n",
            "39977/39977 [==============================] - 5s 115us/sample - loss: 0.7922 - acc: 0.7300 - val_loss: 0.7963 - val_acc: 0.7367\n",
            "Epoch 9/10\n",
            "39977/39977 [==============================] - 5s 117us/sample - loss: 0.7889 - acc: 0.7300 - val_loss: 0.7961 - val_acc: 0.7367\n",
            "Epoch 10/10\n",
            "39977/39977 [==============================] - 5s 117us/sample - loss: 0.7859 - acc: 0.7300 - val_loss: 0.8016 - val_acc: 0.7367\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp9NlWnPwVIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get back the encoding values to compare with the predictions for getting accuracy score\n",
        "i, val = np.where(y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6HL1V8MwW7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get predictions for validation data\n",
        "pred = model.predict_classes(x_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3USGFLxJwYhf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c29190bf-a606-4283-d2d3-8ca0934eb90a"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print ('Model Accuracy: ', accuracy_score(val, pred))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Accuracy:  0.7366683341670835\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R47A6Ysfev3l",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}